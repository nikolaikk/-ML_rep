import torchimport torch.nn as nnimport matplotlib.pyplot as pltimport pandas as pdimport numpy as np###################### Set parameters###################### Data paramsnoise_var = 0num_datapoints = 100test_size = 0.2num_train = 4# Network paramsinput_size = 20# If `per_element` is True, then LSTM reads in one timestep at a time.per_element = Trueif per_element:    lstm_input_size = 1else:    lstm_input_size = input_size    # size of hidden layersh1 = 32output_dim = 1num_layers = 2learning_rate = 1e-3num_epochs = 500dtype = torch.float###################### Generate data#####################dataset_train = pd.read_csv("https://raw.githubusercontent.com/mwitiderrick/stockprice/master/NSE-TATAGLOBAL.csv")dataset_train = pd.read_csv("https://raw.githubusercontent.com/plotly/datasets/master/monthly-milk-production-pounds.csv")training_set = dataset_train.iloc[:, 1].values.reshape(-1,1)dataset_train.head()# Feature Scalingfrom sklearn.preprocessing import MinMaxScalersc = MinMaxScaler(feature_range = (0, 1))training_set_scaled = sc.fit_transform(training_set)# Creating a data structure with 60 timesteps and 1 outputX_train = []y_train = []train_window = 20for i in range(train_window, 168):    X_train.append(training_set_scaled[i-train_window:i, 0])    y_train.append(training_set_scaled[i, 0])    X_train, y_train = np.array(X_train), np.array(y_train)# ReshapingX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))X_train = torch.Tensor(X_train)y_train = torch.Tensor(y_train)###################### Build model###################### Here we define our model as a classclass LSTM(nn.Module):    def __init__(self, input_dim, hidden_dim, batch_size, output_dim=1,                    num_layers=2):        super(LSTM, self).__init__()        self.input_dim = input_dim        self.hidden_dim = hidden_dim        self.batch_size = batch_size        self.num_layers = num_layers        # Define the LSTM layer        self.lstm = nn.LSTM(input_size=self.input_dim,                             hidden_size=self.hidden_dim,                             num_layers=self.num_layers)        # Define the output layer        self.linear = nn.Linear(self.hidden_dim, output_dim)    def init_hidden(self):        # This is what we'll initialise our hidden state as        return (torch.zeros(self.num_layers, self.batch_size, self.hidden_dim),                torch.zeros(self.num_layers, self.batch_size, self.hidden_dim))    def forward(self, input):        # Forward pass through LSTM layer        # shape of lstm_out: [input_size, batch_size, hidden_dim]        # shape of self.hidden: (a, b), where a and b both         # have shape (num_layers, batch_size, hidden_dim).        lstm_out, self.hidden = self.lstm(input.view(len(input), self.batch_size, -1))                # Only take the output from the final timetep        # Can pass on the entirety of lstm_out to the next layer if it is a seq2seq prediction        y_pred = self.linear(lstm_out[-1].view(self.batch_size, -1))        return y_pred.view(-1)model = LSTM(lstm_input_size, h1, batch_size=num_train, output_dim=output_dim, num_layers=num_layers)loss_fn = torch.nn.MSELoss(size_average=False)optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)###################### Train model#####################hist = np.zeros(num_epochs)for t in range(num_epochs):    # Initialise hidden state    # Don't do this if you want your LSTM to be stateful    model.hidden = model.init_hidden()        # Forward pass    y_pred = model(X_train)    loss = loss_fn(y_pred, y_train)    if t % 100 == 0:        print("Epoch ", t, "MSE: ", loss.item())    hist[t] = loss.item()    # Zero out gradient, else they will accumulate between epochs    optimiser.zero_grad()    # Backward pass    loss.backward()    # Update parameters    optimiser.step()###################### Plot preds and performance#####################plt.plot(y_pred.detach().numpy(), label="Preds")plt.plot(y_train.detach().numpy(), label="Data")plt.legend()plt.show()plt.plot(hist, label="Training loss")plt.legend()plt.show()